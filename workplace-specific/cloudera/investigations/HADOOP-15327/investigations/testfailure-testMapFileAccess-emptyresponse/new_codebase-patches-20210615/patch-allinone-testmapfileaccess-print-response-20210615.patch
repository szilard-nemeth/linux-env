Index: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/test/java/org/apache/hadoop/mapred/TestShuffleHandler.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/test/java/org/apache/hadoop/mapred/TestShuffleHandler.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/test/java/org/apache/hadoop/mapred/TestShuffleHandler.java
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/test/java/org/apache/hadoop/mapred/TestShuffleHandler.java	(revision c9129e2f57e588dc37f345e169d54d6f525499f9)
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/test/java/org/apache/hadoop/mapred/TestShuffleHandler.java	(date 1623759403923)
@@ -42,11 +42,13 @@
 import static org.apache.hadoop.test.MetricsAsserts.getMetrics;
 import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.fail;
 import static org.junit.Assume.assumeTrue;
 import static org.mockito.ArgumentMatchers.anyString;
 import static org.mockito.Mockito.mock;
 import static org.mockito.Mockito.when;
 
+import java.io.ByteArrayOutputStream;
 import java.io.DataInputStream;
 import java.io.EOFException;
 import java.io.File;
@@ -54,10 +56,13 @@
 import java.io.FileOutputStream;
 import java.io.IOException;
 import java.net.HttpURLConnection;
+import java.net.InetSocketAddress;
+import java.net.Proxy;
 import java.net.URL;
 import java.net.SocketAddress;
 import java.nio.ByteBuffer;
 import java.nio.channels.ClosedChannelException;
+import java.nio.charset.StandardCharsets;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
@@ -101,7 +106,6 @@
 import org.apache.hadoop.yarn.server.api.AuxiliaryLocalPathHandler;
 import org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer;
 import org.apache.hadoop.yarn.server.records.Version;
-import org.hamcrest.CoreMatchers;
 import org.junit.Assert;
 import org.junit.Test;
 import org.mockito.invocation.InvocationOnMock;
@@ -153,6 +157,7 @@
       mapOutputSender = new MapOutputSender(this, headerWriteCount, lastSocketAddress, shuffleHeaderProvider);
       int headerSize = getShuffleHeaderSize(shuffleHeaderProvider);
       this.expectedResponseSize = headerWriteCount * headerSize;
+      setUseOutboundExceptionHandler(true);
     }
 
     private int getShuffleHeaderSize(ShuffleHeaderProvider shuffleHeaderProvider) throws IOException {
@@ -312,7 +317,15 @@
     private String getExecutingMethodName() {
       StackTraceElement[] stackTrace = Thread.currentThread()
           .getStackTrace();
-      String methodName = stackTrace[1].getMethodName();
+      // Array items (indices):
+      // 0: java.lang.Thread.getStackTrace(...)
+      // 1: TestShuffleHandler$LoggingHttpResponseEncoder.getExecutingMethodName(...)
+      String methodName = stackTrace[2].getMethodName();
+      //If this method was called from printExecutingMethod, 
+      // we have yet another stack frame
+      if (methodName.endsWith("printExecutingMethod")) {
+        methodName = stackTrace[3].getMethodName();
+      }
       String className = this.getClass().getSimpleName();
       return className + "#" + methodName;
     }
@@ -436,7 +449,7 @@
       try {
         this.responseCode = conn.getResponseCode();
       } catch (IOException e) {
-        Assert.fail("Failed to read response code from connection: " + conn);
+        fail("Failed to read response code from connection: " + conn);
       }
     }
 
@@ -517,7 +530,7 @@
         LOG.debug("Opened DataInputStream for connection: {}/{}", (reqIdx + 1), requests);
         ShuffleHeader header = new ShuffleHeader();
         header.readFields(input);
-        int sumReadBytes = readDataFromInputStream(input);
+        int sumReadBytes = readDataFromInputStreamAndReturnBytesRead(input);
         connectionData.add(HttpConnectionData
             .create(conn, sumReadBytes, lastSocketAddress.getSocketAddres()));
         input.close();
@@ -538,18 +551,34 @@
       return connectionData.get(i);
     }
 
-    private int readDataFromInputStream(DataInputStream input) throws IOException {
-      byte[] buffer = new byte[1024];
-      int sumReadBytes = 0;
-      int read;
-      while ((read = input.read(buffer)) != -1) {
-        sumReadBytes += read;
-      }
-      LOG.debug("***Read bytes: " + sumReadBytes);
-      return sumReadBytes;
+  }
+
+  private static int readDataFromInputStreamAndReturnBytesRead(DataInputStream input) throws IOException {
+    byte[] buffer = new byte[1024];
+    int sumReadBytes = 0;
+    int read;
+    while ((read = input.read(buffer)) != -1) {
+      sumReadBytes += read;
+    }
+    LOG.debug("Read bytes: " + sumReadBytes);
+    return sumReadBytes;
+  }
+
+  private static String readDataFromInputStream(DataInputStream input) throws IOException {
+    ByteArrayOutputStream buffer = new ByteArrayOutputStream();
+    int ret;
+    int bytesRead = 0;
+    byte[] data = new byte[1024];
+    while ((ret = input.read(data)) != -1) {
+      buffer.write(data);
+      bytesRead += ret;
     }
+    LOG.debug("Read bytes: " + bytesRead);
+    buffer.flush();
+
+    return new String(buffer.toByteArray(), StandardCharsets.UTF_8);
   }
-
+  
   private int getKeepAliveTimeout() {
     if (DEBUG_FRIENDLY_MODE) {
       return DEBUG_FRIENDLY_KEEP_ALIVE;
@@ -561,6 +590,7 @@
     final ArrayList<Throwable> failures = new ArrayList<>();
 
     public ShuffleHandlerForTests() {
+      setUseOutboundExceptionHandler(true);
     }
 
     public ShuffleHandlerForTests(MetricsSystem ms) {
@@ -584,6 +614,15 @@
   class MockShuffleHandler extends org.apache.hadoop.mapred.ShuffleHandler {
     final ArrayList<Throwable> failures = new ArrayList<>();
 
+    public MockShuffleHandler() {
+      setUseOutboundExceptionHandler(true);
+    }
+
+    public MockShuffleHandler(MetricsSystem ms) {
+      super(ms);
+      setUseOutboundExceptionHandler(true);
+    }
+
     private AuxiliaryLocalPathHandler pathHandler =
         new TestAuxiliaryLocalPathHandler();
     @Override
@@ -671,6 +710,15 @@
       org.apache.hadoop.mapred.ShuffleHandler {
     final ArrayList<Throwable> failures = new ArrayList<>(1);
 
+    public MockShuffleHandler2() {
+      setUseOutboundExceptionHandler(true);
+    }
+
+    public MockShuffleHandler2(MetricsSystem ms) {
+      super(ms);
+      setUseOutboundExceptionHandler(true);
+    }
+
     boolean socketKeepAlive = false;
     @Override
     protected Shuffle getShuffle(final Configuration conf) {
@@ -1052,6 +1100,7 @@
         };
       }
     };
+    shuffleHandler.setUseOutboundExceptionHandler(true);
     shuffleHandler.init(conf);
     shuffleHandler.start();
 
@@ -1084,9 +1133,9 @@
       } catch (IOException ioe) {
         LOG.info("Expected - connection should not be open");
       } catch (NumberFormatException ne) {
-        Assert.fail("Expected a numerical value for RETRY_AFTER header field");
+        fail("Expected a numerical value for RETRY_AFTER header field");
       } catch (Exception e) {
-        Assert.fail("Expected a IOException");
+        fail("Expected a IOException");
       }
       int statusCode = conn.getResponseCode();
       LOG.debug("Connection status code: {}", statusCode);
@@ -1138,13 +1187,13 @@
    *
    * @throws Exception exception
    */
-  @Test(timeout = 100000)
+  @Test(timeout = 10000000)
   public void testMapFileAccess() throws IOException {
     final ArrayList<Throwable> failures = new ArrayList<>();
     // This will run only in NativeIO is enabled as SecureIOUtils need it
-    assumeTrue(NativeIO.isAvailable());
+//    assumeTrue(NativeIO.isAvailable());
     Configuration conf = new Configuration();
-    conf.setInt(ShuffleHandler.SHUFFLE_PORT_CONFIG_KEY, 0);
+    conf.setInt(ShuffleHandler.SHUFFLE_PORT_CONFIG_KEY, 8088);
     conf.setInt(ShuffleHandler.MAX_SHUFFLE_CONNECTIONS, 3);
     conf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION,
         "kerberos");
@@ -1178,10 +1227,17 @@
             failures.add(cause);
             super.exceptionCaught(ctx, cause);
           }
+
+          @Override
+          public void channelActive(ChannelHandlerContext ctx) throws Exception {
+            ctx.pipeline().replace(HttpResponseEncoder.class, "loggingResponseEncoder", new LoggingHttpResponseEncoder(false));
+            super.channelActive(ctx);
+          }
         };
       }
     };
     AuxiliaryLocalPathHandler pathHandler = new TestAuxiliaryLocalPathHandler();
+    shuffleHandler.setUseOutboundExceptionHandler(true);
     shuffleHandler.setAuxiliaryLocalPathHandler(pathHandler);
     shuffleHandler.init(conf);
     try {
@@ -1203,31 +1259,71 @@
                       ShuffleHandler.SHUFFLE_PORT_CONFIG_KEY)
                   + "/mapOutput?job=job_12345_0001&reduce=" + reducerId
                   + "&map=attempt_12345_1_m_1_0");
-      HttpURLConnection conn = (HttpURLConnection) url.openConnection();
+      ///PROXY CONFIG
+//      Assert.assertEquals("localhost", System.getProperty("http.proxyHost"));
+//      Assert.assertEquals("8888", System.getProperty("http.proxyPort"));
+//      Assert.assertEquals("localhost", System.getProperty("https.proxyHost"));
+//      Assert.assertEquals("8888", System.getProperty("https.proxyPort"));
+//      Assert.assertEquals(null, System.getProperty("http.nonProxyHosts"));
+//      System.setProperty("http.proxyPort", "1234");
+//      System.setProperty("http.proxyHost", "127.0.0.1 ");
+//      System.setProperty("http.proxyHost", "localhost");
+//      System.setProperty("http.proxyPort", "8888");
+      
+      
+//      HttpURLConnection conn = (HttpURLConnection) url.openConnection();
+      Proxy webProxy
+          = new Proxy(Proxy.Type.HTTP, new InetSocketAddress("127.0.0.1", 8888));
+      HttpURLConnection conn
+          = (HttpURLConnection) url.openConnection(webProxy);
+      
       conn.setRequestProperty(ShuffleHeader.HTTP_HEADER_NAME,
           ShuffleHeader.DEFAULT_HTTP_HEADER_NAME);
       conn.setRequestProperty(ShuffleHeader.HTTP_HEADER_VERSION,
           ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION);
       conn.connect();
+      //====================================
+      //READ: Old way
+      String receivedString;
+      DataInputStream is = null;
       byte[] byteArr = new byte[10000];
       try {
-        DataInputStream is = new DataInputStream(conn.getInputStream());
+        is = new DataInputStream(conn.getInputStream());
         is.readFully(byteArr);
       } catch (EOFException e) {
         // ignore
       }
+      receivedString = new String(byteArr, StandardCharsets.UTF_8);
+      //====================================
+      //READ: New way
+//      DataInputStream input;
+//      if (conn.getResponseCode() == HttpURLConnection.HTTP_OK) {
+//        LOG.debug("Connection was OK");
+//        input = new DataInputStream(conn.getInputStream());
+//      } else {
+//        LOG.debug("Connection was NOT OK");
+//        input = new DataInputStream(conn.getErrorStream());
+//      }
+//      receivedString = readDataFromInputStream(input);
+      //====================================
+      //COMMON
+      //TODO Add this to old codebase + check results: Guarantee that HTTP 200 OK was received in each testcase
+      Assert.assertEquals(HttpURLConnection.HTTP_OK, conn.getResponseCode());
+      System.out.println("received: " + receivedString);
+      Assert.assertNotEquals("", receivedString);
+      //TODO check how does this work with tcpdump
+//      Assert.assertTrue(String.format("Received string '%s' should contain " +
+//          "message '%s'", receivedString, message),
+//          receivedString.contains(message));
+
       // Retrieve file owner name
-      FileInputStream is = new FileInputStream(fileMap.get(0));
-      String owner = NativeIO.POSIX.getFstat(is.getFD()).getOwner();
-      is.close();
-
-      String message =
-          "Owner '" + owner + "' for path " + fileMap.get(0).getAbsolutePath()
-              + " did not match expected owner '" + user + "'";
-      String receivedString = new String(byteArr);
-      Assert.assertTrue(String.format("Received string '%s' should contain " +
-          "message '%s'", receivedString, message),
-          receivedString.contains(message));
+//      FileInputStream is = new FileInputStream(fileMap.get(0));
+//      String owner = NativeIO.POSIX.getFstat(is.getFD()).getOwner();
+//      is.close();
+//
+//      String message =
+//          "Owner '" + owner + "' for path " + fileMap.get(0).getAbsolutePath()
+//              + " did not match expected owner '" + user + "'";
     } finally {
       shuffleHandler.stop();
       FileUtil.fullyDelete(ABS_LOG_DIR);
@@ -1439,7 +1535,7 @@
     
       try {
         shuffle.start();
-        Assert.fail("Incompatible version, should expect fail here.");
+        fail("Incompatible version, should expect fail here.");
       } catch (ServiceStateException e) {
         Assert.assertTrue("Exception message mismatch", 
         e.getMessage().contains("Incompatible version for state DB schema:"));
@@ -1535,6 +1631,7 @@
         };
       }
     };
+    shuffleHandler.setUseOutboundExceptionHandler(true);
     shuffleHandler.setAuxiliaryLocalPathHandler(pathHandler);
     shuffleHandler.init(conf);
     try {
Index: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java	(revision c9129e2f57e588dc37f345e169d54d6f525499f9)
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java	(date 1623758297757)
@@ -797,6 +797,11 @@
     }
   }
 
+  @VisibleForTesting
+  public void setUseOutboundExceptionHandler(boolean useHandler) {
+    this.useOutboundExceptionHandler = useHandler;
+  }
+
   static class TimeoutHandler extends IdleStateHandler {
     private final int connectionKeepAliveTimeOut;
     private boolean enabledTimeout;
@@ -1069,6 +1074,7 @@
         // This seems like a bug combined with bad expectations in the tests.
         // See details in jira
         ch.writeAndFlush(response);
+        ch.writeAndFlush(LastHttpContent.EMPTY_LAST_CONTENT);
         LOG.error("Shuffle error in populating headers :", e);
         String errorMessage = getErrorMessage(e);
         sendError(ctx,errorMessage , INTERNAL_SERVER_ERROR);
@@ -1388,13 +1394,14 @@
       response.headers().set(ShuffleHeader.HTTP_HEADER_VERSION,
           ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION);
       for (Map.Entry<String, String> header : headers.entrySet()) {
+        LOG.debug("Header: " + header);
         response.headers().set(header.getKey(), header.getValue());
       }
-
+      LOG.debug("Closing connection");
       // Close the connection as soon as the error message is sent.
       ctx.channel().writeAndFlush(response).addListener(ChannelFutureListener.CLOSE);
     }
-
+    
     @Override
     public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause)
         throws Exception {
