import re
from typing import List, Dict, Tuple, Optional

# Sample output structure from the Bash script (size, filename)
SAMPLE_DATA = """
 6.3M  README.md
 1.2K   src/utils.py
 1.7G   large/data.bin
 128B   .gitignore
 5.1K   build/log.txt
 22.5M  assets/model.zip
"""

def parse_human_size(size_str: str) -> Optional[int]:
    """
    Converts a human-readable size string (e.g., '1.7G', '5.1K', '128B') to bytes.
    It expects IEC units (B, K, M, G, T, P) as generated by numfmt --to=iec.
    """
    size_str = size_str.strip().upper()
    if not size_str:
        return None

    # Regex to capture the number and the unit (B, K, M, G, T, P)
    # The 'B' suffix is often optional in numfmt IEC output but we'll allow it.
    match = re.match(r"(\d+(\.\d+)?)\s*([KMGTPE])?B?", size_str)
    if not match:
        # Handle cases where the size is just a raw number of bytes
        if size_str.isdigit():
            return int(size_str)
        return None

    value = float(match.group(1))
    unit = match.group(3)

    units_map = {
        None: 1,      # Bytes
        'K': 1024,    # Kibibytes
        'M': 1024**2, # Mebibytes
        'G': 1024**3, # Gibibytes
        'T': 1024**4, # Tebibytes
        'P': 1024**5, # Pebibytes
    }

    multiplier = units_map.get(unit, 1)

    return int(value * multiplier)

def analyze_sizes(data: str, top_n: int = 5) -> List[Dict]:
    """
    Processes the raw file size data, sorts it, and returns the top N largest files.

    Args:
        data: A string containing lines of size and filename, separated by whitespace.
        top_n: The number of largest files to return.

    Returns:
        A sorted list of file dictionaries.
    """
    results = []
    lines = data.strip().split('\n')

    for line in lines:
        try:
            # Split the line by whitespace
            parts = line.strip().split(maxsplit=1)
            if len(parts) != 2:
                # Skip lines that don't have exactly two parts (size and filename)
                continue

            human_size, filename = parts
            size_in_bytes = parse_human_size(human_size)

            if size_in_bytes is not None:
                results.append({
                    'human_size': human_size.strip(),
                    'size_bytes': size_in_bytes,
                    'filename': filename.strip()
                })

        except Exception as e:
            # Print any parsing errors but continue processing the rest of the data
            print(f"Error parsing line '{line.strip()}': {e}")
            continue

    # Sort the results by 'size_bytes' in descending order
    results.sort(key=lambda x: x['size_bytes'], reverse=True)

    return results[:top_n]

if __name__ == "__main__":
    N = 3
    print(f"--- Top {N} Largest Files in Commit ---")

    # In a real scenario, you would pipe the output of the bash script to this script,
    # or read from stdin (e.g., if you executed the bash script from Python).
    # For demonstration, we use the SAMPLE_DATA.
    
    # Example of how you could read from a file or stdin in a real application:
    # import sys
    # try:
    #     raw_data = sys.stdin.read()
    # except:
    #     raw_data = SAMPLE_DATA
    
    # Using the sample data for demonstration:
    top_results = analyze_sizes(SAMPLE_DATA, top_n=N)

    if top_results:
        # Determine the maximum width for the size column for clean alignment
        max_size_len = max(len(r['human_size']) for r in top_results)
        
        # Output the results
        for i, item in enumerate(top_results):
            # Print rank, human-readable size (left-padded for alignment), and filename
            size_padded = item['human_size'].rjust(max_size_len)
            print(f"#{i+1}: {size_padded} -> {item['filename']}")
    else:
        print("No valid file size data found to process.")
        
    print("-" * (30 + max_size_len))

    # Example of how to use this script by piping data from the shell script (if allowed):
    # $ ./git-commit-size.sh <hash> | python git_commit_size_analyzer.py
