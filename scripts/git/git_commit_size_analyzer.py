import re
import sys
from typing import List, Dict, Tuple, Optional

def parse_human_size(size_str: str) -> Optional[int]:
    """
    Converts a human-readable size string (e.g., '1.7G', '5.1K', '128B') to bytes.
    It expects IEC units (B, K, M, G, T, P) as generated by numfmt --to=iec.
    """
    size_str = size_str.strip().upper()
    if not size_str:
        return None

    # Regex to capture the number and the unit (B, K, M, G, T, P)
    # The 'B' suffix is often optional in numfmt IEC output but we'll allow it.
    match = re.match(r"(\d+(\.\d+)?)\s*([KMGTPE])?B?", size_str)
    if not match:
        # Handle cases where the size is just a raw number of bytes
        if size_str.isdigit():
            return int(size_str)
        return None

    value = float(match.group(1))
    unit = match.group(3)

    units_map = {
        None: 1,      # Bytes
        'K': 1024,    # Kibibytes
        'M': 1024**2, # Mebibytes
        'G': 1024**3, # Gibibytes
        'T': 1024**4, # Tebibytes
        'P': 1024**5, # Pebibytes
    }

    multiplier = units_map.get(unit, 1)

    return int(value * multiplier)

def analyze_sizes(data: str, top_n: int = 5) -> List[Dict]:
    """
    Processes the raw file size data, sorts it, and returns the top N largest files.

    Args:
        data: A string containing lines of size and filename, separated by whitespace.
        top_n: The number of largest files to return.

    Returns:
        A sorted list of file dictionaries.
    """
    results = []
    lines = data.strip().split('\n')

    for line in lines:
        try:
            # Split the line by whitespace
            parts = line.strip().split(maxsplit=1)
            if len(parts) != 2:
                # Skip lines that don't have exactly two parts (size and filename)
                continue

            human_size, filename = parts
            size_in_bytes = parse_human_size(human_size)

            if size_in_bytes is not None:
                results.append({
                    'human_size': human_size.strip(),
                    'size_bytes': size_in_bytes,
                    'filename': filename.strip()
                })

        except Exception as e:
            # Print any parsing errors but continue processing the rest of the data
            print(f"Error parsing line '{line.strip()}': {e}")
            continue

    # Sort the results by 'size_bytes' in descending order
    results.sort(key=lambda x: x['size_bytes'], reverse=True)

    return results[:top_n]

if __name__ == "__main__":
    N = 5 # Default to show top 5, user can change this line if they want a different N

    # 1. Check for the required file argument
    if len(sys.argv) != 2:
        print(f"Usage: python {sys.argv[0]} <path_to_size_file>")
        sys.exit(1)

    input_filepath = sys.argv[1]

    # 2. Read the data from the specified file
    try:
        with open(input_filepath, 'r') as f:
            raw_data = f.read()
    except FileNotFoundError:
        print(f"Error: The file '{input_filepath}' was not found.")
        sys.exit(1)
    except Exception as e:
        print(f"An error occurred while reading the file: {e}")
        sys.exit(1)

    print(f"--- Analyzing Commit Size Data from '{input_filepath}' (Top {N}) ---")

    # 3. Analyze and get results
    top_results = analyze_sizes(raw_data, top_n=N)

    if top_results:
        # Determine the maximum width for the size column for clean alignment
        max_size_len = max(len(r['human_size']) for r in top_results)

        # Output the results
        for i, item in enumerate(top_results):
            # Print rank, human-readable size (left-padded for alignment), and filename
            size_padded = item['human_size'].rjust(max_size_len)
            print(f"#{i+1}: {size_padded} -> {item['filename']}")
    else:
        print("No valid file size data found to process.")

    print("-" * 50)
    print("Run Example: python commit_size_analyzer.py my_commit_data.txt")
