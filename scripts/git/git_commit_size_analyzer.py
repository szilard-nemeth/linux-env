import re
import sys
from typing import List, Dict, Tuple, Optional

def convert_bytes_to_human_readable(bytes: int):
    return f"{bytes / 1024 ** 3:.2f} GB" if bytes > 1024 ** 3 else f"{bytes / 1024 ** 2:.2f} MB"


def parse_human_size(size_str: str) -> Optional[int]:
    """
    Converts a human-readable size string (e.g., '1.7G', '5.1K', '128B') to bytes.
    It expects IEC units (B, K, M, G, T, P) as generated by numfmt --to=iec.
    """
    size_str = size_str.strip().upper()
    if not size_str:
        return None

    # Regex to capture the number and the unit (B, K, M, G, T, P)
    # The 'B' suffix is often optional in numfmt IEC output but we'll allow it.
    match = re.match(r"(\d+(\.\d+)?)\s*([KMGTPE])?B?", size_str)
    if not match:
        # Handle cases where the size is just a raw number of bytes
        if size_str.isdigit():
            return int(size_str)
        return None

    value = float(match.group(1))
    unit = match.group(3)

    units_map = {
        None: 1,      # Bytes
        'K': 1024,    # Kibibytes
        'M': 1024**2, # Mebibytes
        'G': 1024**3, # Gibibytes
        'T': 1024**4, # Tebibytes
        'P': 1024**5, # Pebibytes
    }

    multiplier = units_map.get(unit, 1)

    return int(value * multiplier)

def analyze_sizes(data: str) -> List[Dict]:
    """
    Processes the raw file size data, sorts it, and returns the top N largest files.

    Args:
        data: A string containing lines of size and filename, separated by whitespace.

    Returns:
        A sorted list of file dictionaries.
    """
    results = []
    lines = data.strip().split('\n')

    for line in lines:
        try:
            # Split the line by whitespace
            parts = line.strip().split(maxsplit=1)
            if len(parts) != 2:
                # Skip lines that don't have exactly two parts (size and filename)
                continue

            human_size, filename = parts
            size_in_bytes = parse_human_size(human_size)

            if size_in_bytes is not None:
                results.append({
                    'human_size': human_size.strip(),
                    'size_bytes': size_in_bytes,
                    'filename': filename.strip()
                })

        except Exception as e:
            # Print any parsing errors but continue processing the rest of the data
            print(f"Error parsing line '{line.strip()}': {e}")
            continue

    # Sort the results by 'size_bytes' in descending order
    results.sort(key=lambda x: x['size_bytes'], reverse=True)

    return results

def write_to_temp_file(results):
    import tempfile

    max_size_len = max(len(r['human_size']) for r in results)

    # Create a named temporary file in text mode ('w+t')
    # 'delete=False' prevents immediate deletion when the file is closed,
    # allowing you to access it by its name after closing.
    with tempfile.NamedTemporaryFile(mode='w+t', delete=False, prefix="git-commit-analyzer-all-sorted-") as temp_file:
        for i, item in enumerate(results):
            # Print rank, human-readable size (left-padded for alignment), and filename
            size_padded = item['human_size'].rjust(max_size_len)
            temp_file.write(f"#{i+1}: {size_padded} -> {item['filename']}\n")

        # Get the name of the temporary file
        temp_file_name = temp_file.name

    return temp_file_name

if __name__ == "__main__":
    top_n = 200 # Default to show top 200, user can change this line if they want a different N

    # 1. Check for the required file argument
    if len(sys.argv) != 2:
        print(f"Usage: python {sys.argv[0]} <path_to_size_file>")
        # print("-" * 50)
        # print("Run Example: python commit_size_analyzer.py my_commit_data.txt")
        sys.exit(1)

    input_filepath = sys.argv[1]

    # 2. Read the data from the specified file
    try:
        with open(input_filepath, 'r') as f:
            raw_data = f.read()
    except FileNotFoundError:
        print(f"Error: The file '{input_filepath}' was not found.")
        sys.exit(1)
    except Exception as e:
        print(f"An error occurred while reading the file: {e}")
        sys.exit(1)

    print(f"--- Analyzing Commit Size Data from '{input_filepath}' (Top {top_n}) ---")

    # 3. Analyze and get results
    results = analyze_sizes(raw_data)
    temp_file_name = write_to_temp_file(results)

    top_results = results[:top_n]

    if top_results:
        # Determine the maximum width for the size column for clean alignment
        max_size_len = max(len(r['human_size']) for r in top_results)

        # Output the results
        for i, item in enumerate(top_results):
            # Print rank, human-readable size (left-padded for alignment), and filename
            size_padded = item['human_size'].rjust(max_size_len)
            print(f"#{i+1}: {size_padded} -> {item['filename']}")
    else:
        print("No valid file size data found to process.")

    print("-" * 50)
    print(f"Temporary file with all results ordered created at: {temp_file_name}")

    # Print sum
    from functools import reduce
    sum_bytes = reduce(lambda x, y: x + y, map(lambda x: x['size_bytes'], results))
    print(f"Sum size of all files: {convert_bytes_to_human_readable(sum_bytes)}")
